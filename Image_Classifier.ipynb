{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dba8323c",
   "metadata": {},
   "source": [
    "Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f86a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install ipywidgets requests\n",
    "!{sys.executable} -m pip install torch torchvision\n",
    "!{sys.executable} -m pip install numpy pandas matplotlib tqdm pillow --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4234593b",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4539969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for OS and Cloud\n",
    "import os\n",
    "import uuid\n",
    "import shutil\n",
    "import json\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import zipfile\n",
    "import io\n",
    "import requests\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb48160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for Data Processing and Visualization\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from matplotlib.pyplot import imshow\n",
    "from tqdm import tqdm\n",
    "from ipywidgets import IntProgress\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87c73ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning Libraries\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader,random_split\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(0)\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d57c78a",
   "metadata": {},
   "source": [
    "Define Functions for Loss and Accuracy Plot, Transformed Image Plot and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62a9aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stuff(COST, ACC):\n",
    "    \"\"\"\n",
    "    Plots training cost (loss) and validation accuracy on the same figure using two y-axes.\n",
    "    \n",
    "    Parameters:\n",
    "    COST (list or array): Total training loss per iteration (or epoch)\n",
    "    ACC (list or array): Validation accuracy per iteration (or epoch)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a new figure and a primary axis (ax1)\n",
    "    fig, ax1 = plt.subplots()\n",
    "    \n",
    "    # Plot training loss on the primary y-axis (left)\n",
    "    color = 'tab:red'\n",
    "    ax1.plot(COST, color=color)\n",
    "    ax1.set_xlabel('Iteration', color=color)            # Label for x-axis\n",
    "    ax1.set_ylabel('Total Loss', color=color)           # Label for y-axis (left)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)         # Set y-axis tick color\n",
    "\n",
    "    # Create a secondary y-axis (ax2) sharing the same x-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    # Plot validation accuracy on the secondary y-axis (right)\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('Accuracy', color=color)             # Label for y-axis (right)\n",
    "    ax2.plot(ACC, color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    # Adjust layout to prevent y-label clipping\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # Display the combined plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8622aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow_(inp, title=None):\n",
    "    \"\"\"\n",
    "    Displays a tensor image after reversing normalization.\n",
    "    \n",
    "    Parameters:\n",
    "    - inp (Tensor): Image tensor of shape [C, H, W], usually normalized.\n",
    "    - title (str, optional): Title for the image display.\n",
    "    \"\"\"\n",
    "    # Convert from [C, H, W] to [H, W, C] and to NumPy array\n",
    "    inp = inp.permute(1, 2, 0).numpy()\n",
    "    print(\"Image shape:\", inp.shape)\n",
    "\n",
    "    # Undo normalization (ImageNet mean and std)\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "\n",
    "    # Clip values to [0, 1] range for display\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "\n",
    "    # Display image\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # Short pause for GUI update\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5623b5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, validation_loader, criterion, optimizer, n_epochs, print_=True):\n",
    "    loss_list = []\n",
    "    accuracy_list = []\n",
    "    correct = 0\n",
    "    \n",
    "    n_test = len(val_dataset)\n",
    "    accuracy_best = 0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    print(\"The first epoch should take several minutes\")\n",
    "    \n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        loss_sublist = []\n",
    "        for x, y in train_loader:\n",
    "            \n",
    "            x, y = x.to(device), y.to(device)\n",
    "            model.train()\n",
    "            \n",
    "            z = model(x)\n",
    "            loss = criterion(z, y)\n",
    "            loss_sublist.append(loss.item())\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        print(f\"Epochs {epoch + 1} done\")\n",
    "        schedular.step()\n",
    "        \n",
    "        loss_list.append(np.mean(loss_sublist))\n",
    "        \n",
    "        #validation\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x_test, y_test in validation_loader:\n",
    "                x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "                z = model(x_test)\n",
    "                _, yhat = torch.max(z.data, 1)\n",
    "                correct += (yhat == y_test).sum().item()\n",
    "                \n",
    "        accuracy = correct / n_test\n",
    "        accuracy_list.append(accuracy)\n",
    "        \n",
    "        if accuracy > accuracy_best:\n",
    "            accuracy_best = accuracy\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "        if print_:\n",
    "            print(\"Learning rate:\", optimizer.param_groups[0]['lr'])\n",
    "            print(f\"Validation loss (epoch {epoch + 1}): {np.mean(loss_sublist):.4f}\")\n",
    "            print(f\"Validation accuracy (epoch {epoch + 1}): {accuracy:.4f}\")\n",
    "            \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return accuracy_list, loss_list, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382a5421",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"the device type is\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8530849",
   "metadata": {},
   "source": [
    "Image Processing and Load Data for Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824075c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data for Dataset Preparation\n",
    "# URL of the ZIP file\n",
    "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/ExisQFol3hUHktTjm6a51w/final-project-stop-signs-1-2025-04-25-t-06-47-41-058-z.zip\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Open the zip file from the downloaded content\n",
    "    with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:\n",
    "        zip_ref.extractall(\"not_stopandstop\")  # Extract to a target folder\n",
    "    print(\"Download and extraction complete.\")\n",
    "else:\n",
    "    print(\"Failed to download file:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "280d1fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets path\n",
    "source_dir = \"not_stopandstop/final-project-stop-signs-1-2025-04-25-t-06-47-41-058-z\"  # folder containing images and annotation file\n",
    "annotations_file = os.path.join(source_dir, \"_annotations.json\")  # update name if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f73a0260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations\n",
    "with open(annotations_file, \"r\") as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb943110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "train_ratio = 0.9\n",
    "output_dir = \"dataset\"  # Final output root directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbf8302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare label -> image list\n",
    "label_to_images = {}\n",
    "\n",
    "for filename, entry in annotations[\"annotations\"].items():\n",
    "    label = entry[0][\"label\"]\n",
    "    label_to_images.setdefault(label, []).append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3b6542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and split each class into training and validation sets\n",
    "for label, image_list in label_to_images.items():\n",
    "    random.shuffle(image_list)  # Shuffle the list of images to randomize the split\n",
    "    \n",
    "    # Calculate the number of training images (e.g., 90% of total)\n",
    "    train_cutoff = int(len(image_list) * train_ratio)\n",
    "    \n",
    "    # Split the image list into training and validation sets\n",
    "    train_images = image_list[:train_cutoff]\n",
    "    val_images = image_list[train_cutoff:]\n",
    "\n",
    "    # Loop over both splits: 'train' and 'val'\n",
    "    for split, split_images in zip([\"train\", \"val\"], [train_images, val_images]):\n",
    "        \n",
    "        # Create the output directory for the current split and label\n",
    "        # Example: dataset/train/not_stop or dataset/val/stop\n",
    "        out_path = os.path.join(output_dir, split, label)\n",
    "        os.makedirs(out_path, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "        # Copy each image from the source directory to the appropriate split folder\n",
    "        for img_name in split_images:\n",
    "            src = os.path.join(source_dir, img_name)  # Full path to the source image\n",
    "            dst = os.path.join(out_path, img_name)    # Destination path\n",
    "            shutil.copy2(src, dst)  # Copy the image (preserves metadata)\n",
    "\n",
    "# Print completion message once all images are copied\n",
    "print(\"Train/Val split complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d96f5486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a series of transformations to apply to each image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224,0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b512e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datastes from the respective folders.\n",
    "train_dataset = ImageFolder(root=\"dataset/train\", transform=transform)\n",
    "val_dataset = ImageFolder(root=\"dataset/val\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575e9b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the validation datasets and display the first 3 images.\n",
    "\n",
    "i = 0\n",
    "for x, y in val_dataset:\n",
    "    imshow_(x, f\"y = {y}\")\n",
    "    i += 1\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb26d87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the number of Epochs and Batch size.\n",
    "n_epochs = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a63da08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set optimizer hyperparameters.\n",
    "learning_rate = 0.000001\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "deebbf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable learning rate scheduling.\n",
    "lr_scheduler = True\n",
    "base_lr = 0.001\n",
    "max_lr = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3006384e",
   "metadata": {},
   "source": [
    "Load Model and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b224e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameter pretrained to True.\n",
    "model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdcbee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all the parameters to prevent updates during training.\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f51ada0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Find number of classes from the dataset.\n",
    "n_classes = len(train_dataset.classes)\n",
    "print(n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc3861de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repalce the output layer to match the number of classes.\n",
    "model.fc = nn.Linear(512, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ff863e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device type.\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f78abdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function for multi-class classification.\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee7b9b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders for training and validation.\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d422ba67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the optimizer.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae9ff134",
   "metadata": {},
   "outputs": [],
   "source": [
    "if lr_scheduler:\n",
    "    schedular = torch.optim.lr_scheduler.CyclicLR(\n",
    "        optimizer,\n",
    "        base_lr=0.001,\n",
    "        max_lr=0.01,\n",
    "        step_size_up=5,\n",
    "        mode=\"triangular2\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa36993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start time tracking\n",
    "start_datetime = datetime.now()\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "accuracy_list, loss_list, model = train_model(\n",
    "    model, train_loader, validation_loader, criterion, optimizer, n_epochs=n_epochs\n",
    ")\n",
    "\n",
    "# End time tracking\n",
    "end_datetime = datetime.now()\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# Print results\n",
    "print(\"Training completed.\")\n",
    "print(f\"Start Time     : {start_datetime.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"End Time       : {end_datetime.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Elapsed Time   : {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b6cbc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to model.pt\n",
    "torch.save(model.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531982e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss and validation accuracy.\n",
    "plot_stuff(loss_list, accuracy_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c28c1b2",
   "metadata": {},
   "source": [
    "Test Model with any Uploaded Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa9968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/ko-rMe71oPApYpUj2urgFQ/stop-1.jpeg\",\n",
    "    \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/2oSHTMfHikZvnhKypHO9Uw/stop-2.jpeg\",\n",
    "    \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/6GVQqlNsZ83-me4L9DzAIg/not-stop-1.jpeg\",\n",
    "    \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/47aCgskKGqJTYmIkvV6_mA/not-stop-2.jpeg\"\n",
    "]\n",
    "\n",
    "os.makedirs(\"validation_images\", exist_ok=True)\n",
    "\n",
    "for i, url in enumerate(urls, start=1):\n",
    "    response = requests.get(url, timeout=15)\n",
    "    response.raise_for_status()  # ðŸš¨ stops on 404 or other HTTP errors\n",
    "\n",
    "    file_path = f\"validation_images/image_{i}.jpg\"\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    print(f\"Downloaded {file_path} ({len(response.content)} bytes)\")\n",
    "\n",
    "print(\"All images downloaded correctly!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed688b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a trained model for inference.\n",
    "class_names = ['not_stop', 'stop']\n",
    "model = models.resnet18(pretrained=False)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 2)\n",
    "model.load_state_dict(torch.load(\"model.pt\", map_location=torch.device('cpu')))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "01c34fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the same image transformations used during training.\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.2229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fafd5ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations and add batch dimensions.\n",
    "image_path = r\"C:\\Users\\DELL\\OneDrive\\Desktop\\resnet-image-classifier\\validation_images\\image_4.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "input_tensor = transform(image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3e6afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference without tracking gradients.\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_tensor)\n",
    "    predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "    \n",
    "print(f\"The image was classified as: {class_names[predicted_class]}\")\n",
    "\n",
    "# Display results\n",
    "plt.imshow(image)\n",
    "plt.title(f\"Predicted: {class_names[predicted_class]}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
